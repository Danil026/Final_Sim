# Извлечение данных → загрузка данных в СУБД PostgreSQL → визуализация в Metabase
Создана система для работы с данными маркетплейса:
 - данные автоматически загружаются через API каждый день;
 - сохраняются в базе данных PostgreSQL;
 - дашборд в Metabase.

```bash
Final_Sim/
├── ETL/                   # Модуль ETL-пайплайна
│   ├── API_extract.py     # Единый скрипт агрузки данных в базу данных полученных по API             
│   └── py_log.log         # Файл с логами
│
├── Metabase/                      # SQL-запросы и скриншоты дашборда в Metabase
│   ├── SQL_Metabase.sql	   # SQL коды для расчета показателей в Metabase
│   ├── Dashboard.png              # Скриншот дашборда
│   ├── Metabase_Общий.pdf         # Пдф файл с 1 страницей дашборда
│   ├── Metabase_Retention.pdf     # Пдф файл с 2 страницей дашборда          
│   └── Metabase_(Детализация).pdf # Пдф файл с 3 страницей дашборда
│
├── SQL/                           # SQL-код для создания таблиц
│   └── SQL_Create_Table.sql       # DDL-скрипт для создания таблицы в PostgreSQL
│   ├── Sale_market_Tab.png	   # Скриншот таблицы
│   |__
├── Отчет/                         # Финальные отчёты
│   ├── 1_Исследование_ассортиментной_матрицы.pptx
│   ├── 2_Анализ_базы_клиентов.pptx
│   ├── 1_1_Исследование_ассортиментной_матрицы.ipynb   # Jupyter Notebook анализа ассортиментной матрицы
│   └── 2_1_Анализ_базы_клиентов.ipynb            	# Jupyter Notebook анализа клиентской базы
│
├── config.ini                     # Конфигурационный файл необходимый для ETL скрипта
├── .gitignore                     # Файл gitignore для игнорирования файлов и папок
├── requirements.txt               # Список зависимостей Python
│
└── README.md                      # Описание проекта
```
## Основные этапы
- Получить данные через API покупо на маркетплейсе.
- Развернуть PostgreSQL, создать таблицу и загрузить в нее данные .
- Выполнить историческую загрузку всего требуемого диапазона.
- Загружать новые данные ежедневно в 07:00 за вчерашний день.
- Развернуть Metabase и собрать дашборд по продажам, ассортименту и клиентам.
- Провести 2 исследования на данных 2023 года:
- Исследование ассортиментной матрицы
- Исследование клиентской базы.

## Описание данных

Таблица `sale_market` содержит транзакционные данные маркетплейса:
| Поле | Описание |
|------|-----------|
| `client_id` | ID клиента |
| `gender` | пол клиента |
| `purchase_datetime` | Дата и время покупки |
| `purchase_time_as_seconds_from_midnight` | Количество секунд, прошедших с начала суток |
| `product_id` | ID товара | 
| `quantity` | Количество |
| `price_per_item` | Цена за единицу |
| `discount_per_item` | Скидка |
| `total_price` | Итоговая сумма покупки |

Эти данные формируют данные, необходимые для осуществления анализа в сфере продаж и исследования характеристик клиентской базы

## Подготовка базы данных PostgreSQL
До запуском ETL пайплайна необходимо создать базу данных и таблицы в PostgreSQL:

1. Создайте базу данных
```sql
CREATE DATABASE sale_market;
```
![SQL Table](SQL/Sale_market_Tab.png)
## Установка и запуск
1. Клонируйте репозиторий

```bash
git clone https://github.com/Danil026/Final_Sim.git
cd Final_Sim
```
2. Создайте и активируйте виртуальное окружение
<details> 
<summary>macOS / Linux</summary>

```bash
python3 -m venv venv
source venv/bin/activate
```
</details> 
<details> 
<summary>Windows</summary>

```cmd
python -m venv venv
venv\Scripts\activate
```
</details>

3. Установите зависимости

```bash
pip install -r requirements.txt
```
4. Настройте `config.ini

Образец есть в корневой директории проекта(config-exp.ini)
<details>
<summary>Содержание файла:</summary>
[database]

databname = "имя базы данных"

user = "Имя пользователя"

password = "Пароль"

host = "Хост"

port = Порт

[data_settings]

all_load = 1

date_start =2023-01-01
</details>

all_load - это триггер который определяет режим работы скрипта:

 - 1 - Историческая загрузка данных начиная с date_start

 - 0 - за предыдущий день, игнорирет date_start.

5. Загрузите исторические данные
предварительно нужно поставить значение в файле config.ini
 - all_load = 1
 - date_start =2023-01-01

```bash
python API_extract.py

```
6. Ежедневная загрузка новых данных в базу предварительно нужно поставить значение в файле config.ini
 - all_load = 0
```bash
python API_extract.py
```
## Автоматизация
### Windows:

Используйте планировщик заданий, указав запуск `API_extract.py` в нужные дни.

### Linux/macOS (через `cron`):

1. Откройте редактор crontab:

```bash
crontab -e
```
2. Добавьте строку для ежедневного запуска:

```bash
0 7 * * * /path/to/venv/bin/python /path/to/API_extract.py 
```
## Визуализация в Metabase
[Дашборд →](http://91.201.40.224:3000/dashboard/2-dashbord-prodazh-c-2023-g?%25D0%25B2%25D1%258B%25D0%25B1%25D0%25B5%25D1%2580%25D0%25B8%25D1%2582%25D0%25B5_%25D0%25B4%25D0%25B0%25D1%2582%25D1%2583=&%25D0%25B2%25D1%258B%25D0%25B1%25D0%25B5%25D1%2580%25D0%25B8%25D1%2582%25D0%25B5_id__%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B4%25D1%2583%25D0%25BA%25D1%2582%25D0%25B0=&tab=4-общий)

![Metabase Dashboard](Metabase/Dashboard.png)
Расчеты всех метрик находятся в Metabase/SQL_Metabase.sql
Основные показатели:
- Выручка, количество продаж по дням  
- Скидки и средний чек
- Активность клиентов, частота покупок
- Retention

## Результаты анализа
- Проведён ABC_XYZ-анализ ассортимента, определены категории с низкой рентабельностью.  
- Выполнен когортный и RFM анализ клиентов и рассчитан LTV.  
- Определены пиковые месяцы по выручке и покупательной активности клиентов.  
- Построен дашборд для мониторинга ключевых показателей.

## Стек технологий
- ETL: Python 3.11+: requests / pandas / psycopg2
- Database: PostgreSQL 14+
- Visualization: Metabase
- Scheduling: cron
